# AWS Glue Terraform module

Terraform module which creates AWS Glue resources using both AWS and AWSCC providers.

## Usage

```hcl
module "glue" {
  source = "terraform-aws-modules/glue/aws"

  prefix = "example-"
  # IAM Role
  create_iam_role = true
  iam_role_name   = "example-glue-role"
  # Catalog Database
  create_catalog_database = true
  catalog_database_name   = "example_database"
  # Crawler
  create_crawler = true
  crawler_name   = "example-crawler"
  crawler_s3_targets = [
    {
      path = "s3://example-bucket/example-path"
    }
  ]
  # Job with S3 script upload
  create_job = true
  job_name   = "example-job"
  job_type   = "glueetl"
  glue_version = "4.0"
  # Option 1: Use existing S3 script location
  job_command_script_location = "s3://example-bucket/scripts/example-job.py"
  # Option 2: Upload local script to S3
  create_s3_bucket = true
  job_script_local_path = "${path.module}/scripts/example-job.py"
```

### GlueETL Jobs
```hcl
module "glue_etl_job" {
  source = "terraform-aws-modules/glue/aws"

  # Basic configuration
  prefix = "etl-"
  create_job = true
  job_name = "data-transformation"
  job_type = "glueetl"
  glue_version = "4.0"
  timeout = 60
  max_retries = 2
  
  # Worker configuration
  worker_type = "G.1X"
  number_of_workers = 2
  
  # Autoscaling configuration (only for glueetl jobs)
  enable_autoscaling = true
  
  # Job insights (only for glueetl jobs)
  enable_job_insights = true
  notify_delay_after = 15
  
  # Job parameters
  job_parameters = {
    "--conf" = "spark.dynamicAllocation.enabled=true"
  }
}
```

### PythonShell Jobs
```hcl
module "glue_python_job" {
  source = "terraform-aws-modules/glue/aws"

  # Basic configuration
  prefix = "python-"
  create_job = true
  job_name = "data-processing"
  job_type = "pythonshell"
  
  # PythonShell jobs use max_capacity instead of worker_type/number_of_workers
  max_capacity = 0.0625  # Equivalent to G.025X
  
  # Note: Python version (3.9) is automatically set for PythonShell jobs
  
  # Job parameters
  job_parameters = {
    "--my-param" = "my-value"
  }
}
```
  # Encryption configuration
  enable_s3_encryption = true
  s3_kms_key_arn = "arn:aws:kms:us-east-1:123456789012:key/abcd1234-ab12-cd34-ef56-abcdef123456"
  enable_job_bookmarks_encryption = true
  job_bookmarks_encryption_kms_key_arn = "arn:aws:kms:us-east-1:123456789012:key/abcd1234-ab12-cd34-ef56-abcdef123456"
  enable_cloudwatch_encryption = true
  cloudwatch_encryption_kms_key_arn = "arn:aws:kms:us-east-1:123456789012:key/abcd1234-ab12-cd34-ef56-abcdef123456"
  tags = {
    Environment = "dev"
    Project     = "data-pipeline"
  }
}
```

## Examples

- [Complete](examples/complete) - Complete example with all supported resources
- [Minimal](examples/minimal) - Minimal example with only basic job configuration
- [PythonShell](examples/pythonshell) - Example using PythonShell job type
- [Schema Registry Dependency](examples/schema-registry-dependency) - Example showing schema registry dependencies

## Features

This module supports the following AWS Glue resources:

### AWS Provider Resources
- AWS Glue Catalog Database
- AWS Glue Connection
- AWS Glue Crawler
- AWS Glue Job
- AWS Glue Trigger
- AWS Glue Workflow
- AWS Glue Security Configuration
- AWS Glue Dev Endpoint
- IAM Role for Glue resources
- AWS Glue Schema

### AWSCC Provider Resources
- AWSCC Glue Registry

## Module Configuration

### Common Features

- **Resource Creation Control**: Each resource has a boolean flag to control its creation (e.g., `create_catalog_database`, `create_job`)
- **Resource Naming**: Supports adding a prefix to all resource names via the `prefix` variable
- **Tagging**: Apply consistent tags to all resources via the `tags` variable
- **IAM Role Management**: Option to bring your own IAM role or create one within the module

### IAM Role

The module can either create an IAM role for Glue resources or use an existing one:

- Set `create_iam_role = true` to create a new role (default)
- Set `create_iam_role = false` and provide `iam_role_arn` to use an existing role

### Glue Job Configuration

The module supports creating AWS Glue jobs with various configurations:

#### Job Types

This module currently supports the following AWS Glue job types:
- **GlueETL (`glueetl`)**: For Spark-based ETL jobs
- **PythonShell (`pythonshell`)**: For lightweight Python scripts

> **Note:** Ray jobs are not currently supported by this module.

#### Glue Versions
- **Glue 3.0**: Based on Apache Spark 3.1.1, Python 3.7
- **Glue 4.0**: Based on Apache Spark 3.3.0, Python 3.10
- **Glue 5.0**: Based on Apache Spark 3.4.1, Python 3.11

#### Worker Types
- **Standard**: For general purpose workloads
- **G.1X**: 1 DPU (4 vCPU, 16GB memory)
- **G.2X**: 2 DPU (8 vCPU, 32GB memory)
- **G.4X**: 4 DPU (16 vCPU, 64GB memory)
- **G.8X**: 8 DPU (32 vCPU, 128GB memory)
- **G.025X**: 0.25 DPU (2 vCPU, 4GB memory) - For PythonShell jobs

### Glue Job Script Management

The module supports two approaches for providing Glue job scripts:

#### 1. Direct S3 Path Reference
Provide an existing S3 path to your script:

```hcl
module "glue" {
  source = "terraform-aws-modules/glue/aws"

  # ... other configuration ...
  create_job = true
  job_name   = "example-job"
  job_command_script_location = "s3://my-existing-bucket/scripts/my-job.py"
}
```

#### 2. Local Script Upload
Provide a local script from your repository that will be uploaded to S3:

```hcl
module "glue" {
  source = "terraform-aws-modules/glue/aws"

  # ... other configuration ...
  create_job = true
  job_name   = "example-job"
  
  # Option 1: Create a new S3 bucket for scripts
  create_s3_bucket = true
  s3_bucket_name   = "my-glue-scripts-bucket" # Optional, generated if not provided
  
  # Option 2: Use an existing S3 bucket
  create_s3_bucket      = false
  existing_s3_bucket_name = "my-existing-bucket"
  
  # Local script path and optional S3 key
  job_script_local_path = "${path.module}/scripts/my-job.py"
  job_script_s3_key     = "custom/path/my-job.py" # Optional, defaults to scripts/filename
}
```

This approach automatically:
1. Creates an S3 bucket if requested
2. Uploads your local script to the specified S3 bucket
3. Sets the correct S3 path in the Glue job configuration
4. Tracks script changes using file checksums for proper updates
